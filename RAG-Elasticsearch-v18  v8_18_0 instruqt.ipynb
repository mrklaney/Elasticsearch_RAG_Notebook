{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362d5d5-4c16-4eeb-b150-ade99465f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d157e83-d70b-483c-b3ee-7dd60baf3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Export the API key to an environment variable\n",
    "if not os.path.exists('.env.instruqt'):\n",
    "    env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "    with open('.env.instruqt', 'w') as f:\n",
    "        f.write(env_text)\n",
    "load_dotenv('.env.instruqt')\n",
    "\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4552b-9f24-4618-bd7b-cbb8c67bfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = os.getenv(\"ELASTICSEARCH_URL\", None)\n",
    "es_api_key = os.getenv(\"ELASTICSEARCH_APIKEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1c3fab-008b-4dbf-8ad8-75b3acea83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0ade38-029b-4fc2-ba3c-680ddb790082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for pretty_printing JSON\n",
    "def jsn(x):\n",
    "    import json\n",
    "    x=dict(x)\n",
    "    print(json.dumps(x, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca26ca54-fcf8-4f6f-ba5b-0659122ebbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d12fde2-045e-4484-91c6-da661b6cddd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cluster_name\": \"elasticsearch\",\n",
      "  \"cluster_uuid\": \"0IM995ZESGyT53Q-GPUQRw\",\n",
      "  \"name\": \"Marks-MacBook-Pro.local\",\n",
      "  \"tagline\": \"You Know, for Search\",\n",
      "  \"version\": {\n",
      "    \"build_date\": \"2025-04-08T15:13:46.049795831Z\",\n",
      "    \"build_flavor\": \"default\",\n",
      "    \"build_hash\": \"112859b85d50de2a7e63f73c8fc70b99eea24291\",\n",
      "    \"build_snapshot\": false,\n",
      "    \"build_type\": \"tar\",\n",
      "    \"lucene_version\": \"10.1.0\",\n",
      "    \"minimum_index_compatibility_version\": \"8.0.0\",\n",
      "    \"minimum_wire_compatibility_version\": \"8.18.0\",\n",
      "    \"number\": \"9.0.0\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#connect to Elasticsearch and verify\n",
    "es = Elasticsearch(\n",
    "     hosts=[f\"{es_host}\"],\n",
    "     api_key=es_api_key,\n",
    ")\n",
    "jsn(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e6c9b-e2f8-4637-b213-fb46dacce88e",
   "metadata": {},
   "source": [
    "# Run searches on Elasticsearch #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6405f-7be7-4abe-ac9d-7f5d9cfc303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run a simple query == match \n",
    "def retrieve_documents(query, top_n=2):\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": query\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"elastic_blogs-full-embeddings_e5\", body=search_query)\n",
    "    top_docs = [hit[\"_source\"][\"content\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "    line_separated = \"\\n\\n\".join(top_docs)\n",
    "    print(line_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bd1b9-d1a4-49c0-ae17-a70905139afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_documents(\"Kibana for data analytics\",top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36928de0-2983-4eed-b75c-944936d8fe7a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ec33-b987-4865-9d02-f77a2c2aea1c",
   "metadata": {},
   "source": [
    "That was a simple \"match\" search. We want to be able to run a more sophisticated lexical search on Elasticsearch so we can RAG to the LLM some very relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32b1e8-11c9-45d1-beb1-8684af33524b",
   "metadata": {},
   "source": [
    "The function `create_response` can run searches by calling a search_template (which is more newly a search_application).\n",
    "The search application is running a hybrid search  -  lexical and semantic - combined using RRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e73ae3-f364-4c07-acc4-9366eefcde55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first run with `render_query` to see the hybrid search and check that parameters get filled-in.\n",
    "\n",
    "app_name = \"RAG_application\"                   #search_application built in Kibana Console\n",
    "params1 = {\"query_string\" : \"My first query\",\"size\" : 2}\n",
    "\n",
    "create_response = es.search_application.render_query(name=app_name, params=params1)\n",
    "\n",
    "print(\"The render_query shows the search code is a bool and semantic search combined by RRF: \\n\")\n",
    "jsn(create_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2890-d383-46cd-869f-9221eaf61eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with \"search\" to do a search on Elasticsearch\n",
    "\n",
    "app_name = \"RAG_application\"\n",
    "params1 = {\"query_string\" : \"My first query\", \"size\" : 3}   #dictionary of key:values\n",
    "\n",
    "create_response = es.search_application.search(name=app_name, params=params1)\n",
    "\n",
    "print(\"Documents from running the query: \")\n",
    "jsn(create_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd5b84-85b4-4d51-994b-37e02355a05a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce697b8-40af-47e2-b6db-a7de146c122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve_documemts is a function to run a search template/application\n",
    "def retrieve_documents(query,  top_n=2, search_template=\"RAG_application\"):\n",
    "    params = {\"query_string\": query}\n",
    "    params[\"size\"]=top_n\n",
    "    response = es.search_application.search(name=search_template, params=params)\n",
    "    top_docs = [hit[\"_source\"][\"content\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "    return \"\\n\".join(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac73c2-220a-429c-bbfb-9293413f646c",
   "metadata": {},
   "source": [
    "aside: \n",
    "Later can consider making parameters to constrain time :  \"search_date\": {\"start\": \"now-13y\",  \"end\":\"now\")  <br />\n",
    "and   \"from\"  to page thru results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55db96a-9887-4861-a154-f3caed98805b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#unit test\n",
    "query = \"How can I secure my networks between elasticsearch nodes?\"\n",
    "retrieved_documents = retrieve_documents(query)\n",
    "print(\"Retrieved Documents:\", retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63cdfa-2318-4a7f-aa2d-0ab4c4c125d0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48dfa9-6d4a-485d-b821-adf8e1f65a48",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329b614-4078-4521-85a2-76aea4dda105",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892a06-c8b6-4e12-a5de-8a6ff6b30b40",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184d5fd3-e9c1-44dd-a4fe-b9ad55f0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM is from OpenAI \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69aa7ee8-3fa6-4123-a4ba-8c8b6d14d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with a simple, one-pass interacation with the LLM. The function call2llm takes a systems_prompt, which is the \n",
    "#persona the system assumes in the interaction, and \"users_prompt\" which is the input from the user chatting with the LLM\n",
    "\n",
    "def call2llm(systems_prompt, users_prompt):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": systems_prompt},\n",
    "            {\"role\": \"user\", \"content\": users_prompt}\n",
    "        ],\n",
    "        model=\"gpt-4.1\",\n",
    "        temperature=0.000001  # low means consistent LLM responses (high means more creative)\n",
    "    )\n",
    "    response = response.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00c11ff-ea9f-415d-ab26-bf92b431ce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "llm_answer = call2llm(\"You're a helpful assistant\", \"What is 2+2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25b3aed-1ba0-4281-8e0e-9b7404d3a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you’re referring to a previous sum or calculation, but I don’t have any prior context in this conversation. Could you clarify what you’re referring to or provide more details? I’m here to help!\n"
     ]
    }
   ],
   "source": [
    "llm_answer2 = call2llm(\"You're a helpful assistant\", \"What did we just sum?\")\n",
    "print(llm_answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf9022-2b5c-407a-8af4-2c00bf9a5253",
   "metadata": {},
   "source": [
    "No memory in call2llm of what happened previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f2a66f-d634-4152-a75f-ecda33e5f8ab",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde639e-30c4-48f6-a4ee-6a5a1dc5aa26",
   "metadata": {},
   "source": [
    "#### Implement instead as a python class, which will help in adding conversational memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c97d721-4d35-4536-84ce-8cd9fd96bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "    def call2llm(self, users_prompt, temperature=0.00001):   #low temperature means consistent LLM responses (high means more creative)\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        self.history.append({\"role\": \"user\", \"content\": users_prompt})   #user role prompts the LLM \n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm\n",
    "\n",
    "# This class uses \"old style\" conversational memory technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac7f5910-fb8c-48d0-b0ad-ed0ed456ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.call2llm(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41966b48-e016-44af-ac48-04a137d9dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just asked, \"What is 2 + 2?\"\n"
     ]
    }
   ],
   "source": [
    "llm_answer =  chat.call2llm(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf93b8bb-ca4a-4473-b060-2ce3aa5157a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remembered what you asked because, as an AI assistant, I have access to the ongoing conversation context within this chat session. This allows me to reference previous messages and provide relevant, coherent responses. I don't have memory beyond this session or access to information from other conversations, but within our current chat, I can \"see\" and respond to your earlier questions.\n"
     ]
    }
   ],
   "source": [
    "llm_answer =  chat.call2llm(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3b313-b2eb-4cf9-92f9-9316e6a07512",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2d47e-5ece-4d17-b8ed-d46fe6882ded",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bae89d-62b1-4889-afa3-b2a0d4f6f48f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7a4b7-ff7c-41ed-a8fe-91590c88cc1d",
   "metadata": {},
   "source": [
    "## RAG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f6069a-282a-49f3-8192-5218c8a97d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521b8f4-cc80-44c3-80c7-2636557549c6",
   "metadata": {},
   "source": [
    "Elastic_rag both queries Elastisearch and feeds those docs to the LLM in a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1273b9f4-6d0e-4600-b34e-9e8e9eec4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elastic_rag:\n",
    "    def __init__(self, systems_prompt=\"You are a helpful assistant.\"):                \n",
    "        self.previous_response_id = None                                  #no previous response_id the first time through\n",
    "\n",
    "    #retrieve documents from Elasticsearch\n",
    "    def retrieve(self, query,  top_n=2, search_template=\"RAG_application\"):\n",
    "        params = {\"query_string\": query}\n",
    "        params[\"size\"]=top_n\n",
    "        response = es.search_application.search(name=search_template, params=params)\n",
    "        top_docs = [hit[\"_source\"][\"content\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "        return \"\\n\".join(top_docs)\n",
    "\n",
    "    #combine user's query, conversation history, and docs from Elasticsearch to send to LLM\n",
    "    def augment (self, query, temperature=0.00001, model=\"gpt-4.1\"):\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        retrieval = Elastic_rag()\n",
    "        retrieved = retrieval.retrieve(query)\n",
    "        prompt = ( \"This is the query: \"  +  query +  \" Here are supporting document. Do not summarize the documents. \" + retrieved)\n",
    "        response = client.responses.create(\n",
    "            input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            previous_response_id=self.previous_response_id              #set the previous_id to the current; for memory of conversation\n",
    "        )\n",
    "        self.previous_response_id=response.id                           #update conversation memory with current response.id\n",
    "        return response.output_text \n",
    "\n",
    "# This class uses \"newer style\" conversational memory technique - response_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "521e0536-c8a8-49e6-9e51-ed72ccf95ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is Kibana good for?**\n",
      "\n",
      "Based on the provided documents, Kibana is particularly good for:\n",
      "\n",
      "- **Data Visualization:** Kibana allows users to create a wide variety of charts and graphs from data stored in Elasticsearch. This makes it possible to visually explore and analyze large datasets, such as identifying trends, spikes, or anomalies (e.g., a spike in car models or taxi cabs with over 1 million miles).\n",
      "\n",
      "- **Making Complex Data Understandable:** Even for non-technical users, Kibana provides tools to break down and visualize complex event data, making it easier to draw meaningful conclusions from large and varied datasets.\n",
      "\n",
      "- **Aggregations and Analysis:** With Kibana, users can perform aggregations on their data, such as calculating averages, totals, or breaking down data into buckets (e.g., sales data by day of the week). This helps in understanding not just individual data points, but also broader patterns and metrics.\n",
      "\n",
      "- **Interactive Exploration:** Kibana supports interactive filtering and searching, allowing users to drill down into specific subsets of data (e.g., inverting filters to find rare results in a dataset).\n",
      "\n",
      "- **Hands-on Learning and Customization:** Kibana is highly customizable, offering a range of options for users to tailor their visualizations and dashboards to their specific needs. This flexibility can be both powerful and challenging, as there are many ways to explore and present data.\n",
      "\n",
      "- **Integration with the Elastic Stack:** Kibana works seamlessly with other components of the Elastic Stack (Elasticsearch, Logstash, Beats), making it a central tool for ingesting, searching, and visualizing data.\n",
      "\n",
      "- **User-Friendly for Non-Technical Users:** While Kibana can get technical, it also offers an approachable entry point for users without a programming background, especially with guided training and supportive instructors.\n",
      "\n",
      "- **Continuous Improvement and Security:** Kibana is actively developed, with regular updates that add new features, fix bugs, and address security issues, ensuring a stable and secure experience for users.\n",
      "\n",
      "In summary, Kibana is good for anyone who needs to visualize, explore, and analyze data stored in Elasticsearch, whether for IT operations, business intelligence, or general data exploration, and it is accessible to both technical and non-technical users.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    conversation = Elastic_rag()   # an instance of a conversation\n",
    "\n",
    "    # Adding responses\n",
    "    print(conversation.augment(\"What is Kibana good for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c458da-d07e-4bc0-97b0-3b47b992299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Can I run Kibana in a Docker container?**\n",
      "\n",
      "Yes, you can run Kibana in a Docker container.\n",
      "\n",
      "The supporting documents describe how Elastic provides Docker images and Docker Compose examples that allow you to deploy the full Elastic Stack—including Kibana, Elasticsearch, and Beats—using Docker containers. The process is designed to be as simple as possible, even allowing new users to deploy the entire stack with a single command using Docker Compose. The architecture includes containers for Elasticsearch, Kibana, and various Beats modules, and can be customized or extended as needed.\n",
      "\n",
      "To deploy the stack (including Kibana) using Docker Compose, you would:\n",
      "\n",
      "1. Download and extract the provided archive, which contains Docker Compose files and configuration for each Elastic Stack component.\n",
      "2. Ensure Docker is installed on your system.\n",
      "3. Run the appropriate Docker Compose command for your operating system, such as:\n",
      "   ```\n",
      "   docker-compose -f docker-compose-osx.yml up\n",
      "   ```\n",
      "   or\n",
      "   ```\n",
      "   docker-compose -f docker-compose-linux.yml up\n",
      "   ```\n",
      "4. This command will download the necessary images and start the containers, including Kibana.\n",
      "5. Once the stack is running, you can access Kibana via your browser at the specified URL (e.g., http://localhost:5601).\n",
      "\n",
      "The documents also mention that the setup supports various operating systems (Linux, Windows, OSX), and provides guidance for handling file mounts and port mappings. The example architecture is designed to maximize the number of dashboards and modules available for exploration, and you can further customize the setup by adding more containers or changing configuration options.\n",
      "\n",
      "In summary, Kibana can be run in a Docker container, and Elastic provides official images and Compose files to make this process straightforward.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.augment(\"Can I run Kibana in a Docker container?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "449c2122-83c2-402c-8fde-9a93f163394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first question that you asked me was:  \n",
      "**\"What is Kibana good for?\"**\n"
     ]
    }
   ],
   "source": [
    "print(conversation.augment(\"What was the first question that I asked you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4fd8a13-7320-4714-b89e-235d79f53248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know what you asked me prior because I keep track of the sequence of interactions and questions in our conversation. Each time you ask a question, it is recorded as part of the ongoing context of our exchange. This allows me to reference previous questions and answers, maintain continuity, and provide relevant responses based on the history of our conversation. This process is similar to how structured log data is parsed and analyzed—by keeping track of the sequence and structure, I can refer back to earlier points and maintain context throughout our interaction.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.augment(\"How do you know what I asked you prior?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b08ab4-672b-4ffa-9b75-91cfdebde631",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201e7f8-99a3-494d-ba98-7ff1be8fa347",
   "metadata": {},
   "source": [
    "Here is a conversation where a user fills in an input box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c087ae7-04cb-4cf2-ac66-f928fd60f5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ask your questions about the Elastic Stack.  Be sure to share the background or context of your question.  For example, mention what you want to achieve with Elasticsearch. (e.g., searching, indexing, analytics). Instead of general inquiries, ask specific questions. For instance, 'How do I set up an Elasticsearch index?' or 'What are the best practices for querying in Elasticsearch?' If applicable, provide examples of what you're working on or the challenges you're facing. This helps me tailor my responses to your situation.Indicate how detailed you want the response to be. Are you looking for a brief overview or an in-depth explanation? Let me know if my previous responses were helpful or if there are areas where you need more clarity.\n",
      "\n",
      "\n",
      "Please ask your questions about the Elastic Stack.  Type exit to stop.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation ended.\n"
     ]
    }
   ],
   "source": [
    "print (\"Please ask your questions about the Elastic Stack.  Be sure to share the background or context of your question. \"\n",
    "    \" For example, mention what you want to achieve with Elasticsearch. (e.g., searching, indexing, analytics). \"\n",
    "    \"Instead of general inquiries, ask specific questions. For instance, 'How do I set up an Elasticsearch index?' \"\n",
    "    \"or 'What are the best practices for querying in Elasticsearch?' If applicable, provide examples of what \" \n",
    "    \"you're working on or the challenges you're facing. This helps me tailor my responses to your situation.\"\n",
    "    \"Indicate how detailed you want the response to be. Are you looking for a brief overview or an in-depth \"\n",
    "    \"explanation? Let me know if my previous responses were helpful or if there are areas where you need more \"\n",
    "    \"clarity.\")\n",
    "\n",
    "instance=Elastic_rag()\n",
    "\n",
    "while True:\n",
    "    print (\"\\n\\nPlease ask your questions about the Elastic Stack.  Type exit to stop.\")\n",
    "    user_input = input()\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Conversation ended.\")\n",
    "        break\n",
    "        \n",
    "    #print(\"\\n\\nUser input:\", user_input)\n",
    "    print(\"\\nResponse: \\n\")\n",
    "    llm_answer=instance.augment(user_input)\n",
    "    print(llm_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
