{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3721e39-3250-43c6-8454-10b74276cf0b",
   "metadata": {},
   "source": [
    "\"Notebook magic\" commands to install packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362d5d5-4c16-4eeb-b150-ade99465f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653ea7d-ce5f-43be-8bc8-9cb14057d507",
   "metadata": {},
   "source": [
    "Some imports and environment variables we will make use of to connect to Elasticsearch and OpenAI's LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d157e83-d70b-483c-b3ee-7dd60baf3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Export the API key to an environment variable\n",
    "if not os.path.exists('.env.instruqt'):\n",
    "    env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "    with open('.env.instruqt', 'w') as f:\n",
    "        f.write(env_text)\n",
    "load_dotenv('.env.instruqt')\n",
    "\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base\n",
    "\n",
    "es_host = os.getenv(\"ELASTICSEARCH_URL\", None)\n",
    "es_api_key = os.getenv(\"ELASTICSEARCH_APIKEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b100a70-2575-42e7-b4d5-5283e8c128ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if openai_api_key is None:\n",
    "    raise ValueError(\"The openai_api_key environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ade38-029b-4fc2-ba3c-680ddb790082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Little utility function for pretty printing JSON\n",
    "def jsn(x):\n",
    "    import json\n",
    "    x=dict(x)\n",
    "    print(json.dumps(x, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26ca54-fcf8-4f6f-ba5b-0659122ebbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helps to suppress spurious warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3c966-3f0e-42e4-9df4-3052e1e29b46",
   "metadata": {},
   "source": [
    "Import the Elasticsearch module for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c3fab-008b-4dbf-8ad8-75b3acea83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23139208-c526-43a0-90f4-9054d407106d",
   "metadata": {},
   "source": [
    "Connect to Elasticsearch and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12fde2-045e-4484-91c6-da661b6cddd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "     hosts=[f\"{es_host}\"],\n",
    "     api_key=es_api_key,\n",
    ")\n",
    "jsn(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38ea91-53c1-4f1f-951d-0d7ecd73d68b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e6c9b-e2f8-4637-b213-fb46dacce88e",
   "metadata": {},
   "source": [
    "# Run searches on Elasticsearch #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6405f-7be7-4abe-ac9d-7f5d9cfc303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that runs a simple match query\n",
    "def retrieve_documents(query, top_n=2):\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"body\": query\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"elastic_blogs-full-embeddings_e5\", body=search_query)\n",
    "    top_docs = [hit[\"_source\"][\"body\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "    #top_docs = [hit[\"_source\"][\"content\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "    line_separated = \"\\n\\n\".join(top_docs)\n",
    "    print(line_separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bd1b9-d1a4-49c0-ae17-a70905139afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_documents(\"Kibana for data analytics\",top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce473f-ec2b-4fe3-b12a-f6b5c44c96f7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ec33-b987-4865-9d02-f77a2c2aea1c",
   "metadata": {},
   "source": [
    "That was a simple, but we want to be able to run a more sophisticated lexical search on Elasticsearch so we can RAG to the LLM  more relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32b1e8-11c9-45d1-beb1-8684af33524b",
   "metadata": {},
   "source": [
    "The function `create_response` can run searches by calling a search_template (which is more newly a search_application).\n",
    "The search application is running a hybrid search  -  lexical and semantic - combined using RRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e73ae3-f364-4c07-acc4-9366eefcde55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First run with `render_query` to see the hybrid search and check that parameters get assigned values.\n",
    "\n",
    "app_name = \"RAG_application\"                   #search_application built in Kibana Console\n",
    "params1 = {\"query_string\" : \"My first query\",\"size\" : 2}\n",
    "\n",
    "create_response = es.search_application.render_query(name=app_name, params=params1)\n",
    "\n",
    "print(\"The render_query shows the search code is a bool and semantic search combined by RRF: \\n\")\n",
    "jsn(create_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2890-d383-46cd-869f-9221eaf61eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with \"search\" to do a search on Elasticsearch\n",
    "\n",
    "app_name = \"RAG_application\"\n",
    "params1 = {\"query_string\" : \"My first query\", \"size\" : 3}   #dictionary of key:values\n",
    "\n",
    "create_response = es.search_application.search(name=app_name, params=params1)\n",
    "\n",
    "print(\"Documents from running the query: \")\n",
    "jsn(create_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd5b84-85b4-4d51-994b-37e02355a05a",
   "metadata": {},
   "source": [
    " <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce697b8-40af-47e2-b6db-a7de146c122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve_documemts is a function to run a search template/application\n",
    "def retrieve_documents(query,  top_n=2, search_template=\"RAG_application\"):\n",
    "    params = {\"query_string\": query}\n",
    "    params[\"size\"]=top_n\n",
    "    response = es.search_application.search(name=search_template, params=params)\n",
    "    top_docs = [hit[\"_source\"][\"body\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "    return \"\\n\".join(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55db96a-9887-4861-a154-f3caed98805b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#unit test\n",
    "query = \"How can I secure my networks between elasticsearch nodes?\"\n",
    "retrieved_documents = retrieve_documents(query)\n",
    "print(\"Retrieved Documents:\", retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329b614-4078-4521-85a2-76aea4dda105",
   "metadata": {},
   "source": [
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892a06-c8b6-4e12-a5de-8a6ff6b30b40",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583615eb-5d10-413f-80f2-a65cf808c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5fd3-e9c1-44dd-a4fe-b9ad55f0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM is from OpenAI \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa7ee8-3fa6-4123-a4ba-8c8b6d14d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with a simple, one-pass interacation with the LLM. The function call2llm takes a systems_prompt, which is the \n",
    "#persona the system assumes in the interaction, and \"users_prompt\" which is the input from the user chatting with the LLM\n",
    "\n",
    "def call2llm(systems_prompt, users_prompt):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": systems_prompt},\n",
    "            {\"role\": \"user\", \"content\": users_prompt}\n",
    "        ],\n",
    "        model=\"gpt-4.1\",\n",
    "        temperature=0.000001  # low means consistent LLM responses (high means more creative)\n",
    "    )\n",
    "    response = response.choices[0].message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c11ff-ea9f-415d-ab26-bf92b431ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "llm_answer = call2llm(\"You're a helpful assistant\", \"What is 2+2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b3aed-1ba0-4281-8e0e-9b7404d3a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer2 = call2llm(\"You're a helpful assistant\", \"What did we just sum?\")\n",
    "print(llm_answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf9022-2b5c-407a-8af4-2c00bf9a5253",
   "metadata": {},
   "source": [
    "No memory in call2llm of what happened previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396d44b-1eea-48ad-b9b6-580becf231f1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde639e-30c4-48f6-a4ee-6a5a1dc5aa26",
   "metadata": {},
   "source": [
    "#### Implement instead as a python class, which will help in adding conversational memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97d721-4d35-4536-84ce-8cd9fd96bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "    def call2llm(self, users_prompt, temperature=0.00001):   #low temperature means consistent LLM responses (high means more creative)\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": users_prompt})   #user role prompts the LLM \n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f5910-fb8c-48d0-b0ad-ed0ed456ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.call2llm(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41966b48-e016-44af-ac48-04a137d9dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93b8bb-ca4a-4473-b060-2ce3aa5157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3b313-b2eb-4cf9-92f9-9316e6a07512",
   "metadata": {},
   "source": [
    " <br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7a4b7-ff7c-41ed-a8fe-91590c88cc1d",
   "metadata": {},
   "source": [
    "## RAG solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b5d7-98d8-471e-bfec-93776b027c82",
   "metadata": {},
   "source": [
    "Finally here is the python class that performs our RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521b8f4-cc80-44c3-80c7-2636557549c6",
   "metadata": {},
   "source": [
    "Elastic_rag both queries Elastisearch and feeds those docs to the LLM in a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29612011-c1e7-4e4c-a986-3542daf0209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elasticsearch_rag:\n",
    "    def __init__(self, systems_prompt=\"You are a helpful assistant.\", model=\"gpt-4.1\"):                \n",
    "        #self.previous_response_id = None\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model \n",
    "        self.history = [{\"role\": \"system\", \"content\": systems_prompt}]\n",
    "\n",
    "    #retrieve documents from Elasticsearch\n",
    "    def retrieve(self, query,  top_n=2, search_template=\"RAG_application\"):\n",
    "        params = {\"query_string\": query}\n",
    "        params[\"size\"]=top_n\n",
    "        response = es.search_application.search(name=search_template, params=params)\n",
    "        top_docs = [hit[\"_source\"][\"body\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "        return \"\\n\".join(top_docs)\n",
    "\n",
    "    #combine user's query, conversation history, and docs from Elasticsearch to send to LLM\n",
    "    def augment (self, query, temperature=0.00001):\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        retrieval = Elasticsearch_rag()\n",
    "        retrieved = retrieval.retrieve(query)\n",
    "        prompt = ( \"This is the query: \"  +  query +  \" Here are supporting documents. \" + retrieved)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6e135-61e9-44d0-ab81-8c244aa3163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Elasticsearch_rag()   # an instance of a conversation\n",
    "print(conversation.augment(\"What is Kibana good for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43945a-ef31-481a-be9c-c54bf3f60692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.augment(\"Can I run Kibana in a Docker container?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f66b9-c1be-42f1-9b53-739525ae1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.augment(\"What was the first question I asked?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734d268-a824-4c6f-9800-9f88a39e3e77",
   "metadata": {},
   "source": [
    "Congratulations!  We have examined how to create a RAG application that feeds documents from Elasticsearch to OpenAI's GPT LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
